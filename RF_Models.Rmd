---
title: "Random Forest AMR Prediction"
author: "Arvon Clemons"
date: "9/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries
```{r warning = FALSE, message=FALSE, echo=FALSE}
#install packages only if you have not already done so 
list.of.packages <- c("randomForest", "ggplot2", "dplyr", "caret",
                      "readr", "e1071", "mlbench", "slam","parallel", "doParallel")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
#library packages
for (pkg in c("randomForest", "ggplot2", "dplyr",
              "caret", "readr","parallel", "doParallel")) {
  library(pkg, character.only = TRUE)
  }
```


# Data Cleaning
```{r}
# read metadata.csv with only "Sample_ID" and "*_sr" features
metadata <- read_csv("./metadata.csv") %>% 
  select(Sample_ID,(matches("_sr$")))

# read antibiotic resistance associated unitigs
unitigs_azm <- read_table2("azm_sr_gwas_filtered_unitigs.Rtab")
unitigs_cfx <- read_table2("cfx_sr_gwas_filtered_unitigs.Rtab")
unitigs_cip <- read_table2("cip_sr_gwas_filtered_unitigs.Rtab")

#Function to transpose unitigs table, join metadata, then create .csv file
unitigs <- function(unitig){
  
  # deparse + substitute combo to extract object name
  unitig_name <- deparse(substitute(unitig))
  
  '%>%' <- tidyr::'%>%'
  
  unitig %>%
    # transpose
    tidyr::pivot_longer(-pattern_id, "Sample_ID", "value") %>% 
    tidyr::pivot_wider(Sample_ID, pattern_id) %>%
    # join relevant antibiotic metadata
    dplyr::inner_join(dplyr::select(metadata, "Sample_ID", dplyr::matches(stringr::str_extract(unitig_name, "[[:alpha:]]+$"))), by = "Sample_ID") %>% 
    dplyr::select(dplyr::last_col(), dplyr::everything(), -"Sample_ID") %>% 
    # rename unitigs to format "geneX" where X is a positive integer
    data.table::setnames(., old = names(.[2:length(.)]), new = paste0('gene', seq_along(.[2:length(.)])), skip_absent = T) %>% 
    # relabel into "neg" and "pos"
    mutate(across(where(is.numeric), ~if_else(. == 0, "neg", "pos"))) %>% 
    tidyr::drop_na() %>% 
    # create .csv file
    readr::write_csv(path=paste0('./', stringr::str_extract(unitig_name,"[[:alpha:]]+[_][[:alpha:]]+"), '.csv'))
}

# Manual Transpose, join, and create .csv files of azm unitigs
# unitigs_azm %>%
#   pivot_longer(-pattern_id, "Sample_ID", "value") %>%
#   pivot_wider(Sample_ID, pattern_id) %>%
#   inner_join(select(metadata, "Sample_ID", matches('^azm')), by = "Sample_ID") %>%
#   select('Sample_ID', last_col(), everything()) %>%
#   data.table::setnames(., old = names(.[2:length(.)]), new = paste0('gene', seq_along(.[2:length(.)])), skip_absent = T) %>% 
#   drop_na() %>%
#   write_csv(path="./unitigs_azm.csv")

# Use 'unitigs' function to create .csv files
unitigs(unitigs_azm)
unitigs(unitigs_cfx)
unitigs(unitigs_cip)
```


# Function to create Training and Test datasets
```{r}
createDS <- function(csvname, percent=0.7, seed=05101929){
  stopifnot(is.character(csvname))
  
  '%>%' <- tidyr::'%>%'
  
#Import dataset
rfData <- readr::read_csv(csvname, col_types = readr::cols(.default = readr::col_factor(NULL))) %>% 
  dplyr::select_if(sapply(., nlevels) > 1)
#Split
set.seed(seed)
index <- dplyr::select(rfData,dplyr::matches(stringr::str_extract(unlist(stringr::str_split(csvname, "_"))[2],"^[[:alpha:]]+"))) %>% 
  unlist() %>% 
  caret::createDataPartition(times = 1, p = percent, list = FALSE)

dfTrain <- rfData %>% dplyr::slice(index) #training dataset
dfTest <- rfData %>% dplyr::slice(-index) #testing dataset

#return datasets
dataSets <- list(training = dfTrain, testing = dfTest)
invisible(dataSets)
}

#Example of using createDS()
azmDS <- createDS("./unitigs_azm.csv")
tibble::glimpse(azmDS$training[1:6])
# Observations: 2,435
# Variables: 6
# $ azm_sr <fct> pos, pos, pos, neg, pos, neg, neg, pos, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, n…
# $ gene1  <fct> neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, n…
# $ gene2  <fct> neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, n…
# $ gene3  <fct> neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, n…
# $ gene4  <fct> neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, n…
# $ gene5  <fct> neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, neg, n…
```

# Function to Train and Fit Random Forest Model (USE CAUTION)!!!
```{r}
trainMethods <- function(training, outcome,
                         seed=05101929, workers = 0, ...){
  stopifnot(is.object(training), is.character(outcome))
trCtrl <- caret::trainControl(method = "cv", number = 10, #cross-validation
                       allowParallel = TRUE, #parallelization
                       summaryFunction = twoClassSummary,
                       classProbs = TRUE) #class probabilities

cl <- parallel::makePSOCKcluster(ifelse(workers, workers, parallel::detectCores())) #set number of cores
doParallel::registerDoParallel(cl)
set.seed(seed)

#training and fitting model
  modelRF <- caret::train(as.formula(
    paste0(outcome,"~.")),
    data=training,
    trControl = trCtrl,
    metric = "ROC",
    importance = T,
    ...
  )
parallel::stopCluster(cl)

invisible(modelRF)
}

#Example of using trainMethods()
azmRF <- trainMethods(azmDS$training, "azm_sr", method = "parRF", tuneLength = 15)
azmRF
# Parallel Random Forest 
# 
# 2435 samples
#  490 predictor
#    2 classes: '1', '0' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 2191, 2192, 2191, 2192, 2192, 2192, ... 
# Resampling results across tuning parameters:
# 
#   mtry  Accuracy   Kappa     
#     2   0.8726911  0.01630407
#    36   0.9683870  0.85560386
#    71   0.9733101  0.88246752
#   106   0.9737216  0.88456372
#   141   0.9749511  0.89045073
#   176   0.9745413  0.88863089
#   211   0.9733084  0.88305851
#   245   0.9737182  0.88498361
#   280   0.9720755  0.87744204
#   315   0.9720755  0.87744204
#   350   0.9728968  0.88054653
#   385   0.9712508  0.87334843
#   420   0.9716623  0.87535556
#   455   0.9704294  0.86932190
#   490   0.9696064  0.86605487
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was mtry = 141.
```
# Save Model
```{r}
saveRDS(azmRF, file = "azmRF_model") #save model
```

# Assess model accuracy
```{r}
predictedAR <- predict(azmRF, azmDS$testing) #full predictions
mean(predictedAR == azmDS$testing$azm_sr) #assess full model accuracy
table(predictedAR, azmDS$testing$azm_sr) #confusion matrix
ggplot2::ggplot(azmRF)
```

# Import and Split Data
```{r}
# #Import
# ## May require using "data_clean_script.R" to create "unitigs_cip.csv" in local directory
# cipData <- read_csv("./unitigs_cip.csv", col_types = cols(.default = col_factor(NULL))) %>% 
#   select_if(sapply(., nlevels) > 1)
# #Split
# set.seed(05101929)
# index <- cipData$cip_sr %>% 
#   createDataPartition(times = 1, p = 0.7, list = FALSE)
# 
# cipTrain <- cipData %>% slice(index)
# cipTest <- cipData %>% slice(-index)
```

# Full Model Training and Fit (DO NOT RUN!!!!)
```{r}
# trCtrl <- caret::trainControl("cv", number = 10, #cross-validation
#                        allowParallel = TRUE) #parallelization
# 
# # Select Cores for Parallelization
# cl <- parallel::makePSOCKcluster(4) #set number of cores
# doParallel::registerDoParallel(cl)
# set.seed(05101929)
# 
# #Model Fit
# modelRF <- caret::train(
#   azm_sr~., data = azm$training, method = "parRF",
#   tuneGrid = tunegrid,
#   trControl = trCtrl,
#   importance = TRUE
#   )
# #After fitting model
# 
# saveRDS(modelRF, file = "azmRF_model2") #save full model
# predictedAR <- modelRF %>% predict(azm$testing) #full predictions
# parallel::stopCluster(cl)
# mean(predictedAR == azm$testing$azm_sr) #assess full model accuracy
# 
# # Best tuning parameter
# modelRF$bestTune
```
