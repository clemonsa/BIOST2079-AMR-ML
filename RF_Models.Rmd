---
title: "Random Forest AMR Prediction"
author: "Arvon Clemons"
date: "9/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries
```{r warning = FALSE, message=FALSE, echo=FALSE}
#install packages only if you have not already done so 
list.of.packages <- c("randomForest", "ggplot2", "dplyr", "caret",
                      "readr", "e1071", "mlbench", "slam","parallel", "doParallel")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
#library packages
for (pkg in c("randomForest", "ggplot2", "dplyr",
              "caret", "readr","parallel", "doParallel")) {
  library(pkg, character.only = TRUE)
  }
```

# Function to create Training and Test datasets
```{r}
createDS <- function(csvname, percent=0.7, seed=05101929){
  stopifnot(is.character(csvname))
  
  '%>%' <- tidyr::'%>%'
  
#Import dataset
rfData <- readr::read_csv(csvname, col_types = readr::cols(.default = readr::col_factor(NULL))) %>% 
  dplyr::select_if(sapply(., nlevels) > 1)
#Split
set.seed(seed)
index <- dplyr::select(rfData,dplyr::matches(stringr::str_extract(unlist(stringr::str_split(csvname, "_"))[2],"^[[:alpha:]]+"))) %>% 
  unlist() %>% 
  caret::createDataPartition(times = 1, p = percent, list = FALSE)

dfTrain <- rfData %>% dplyr::slice(index) #training dataset
dfTest <- rfData %>% dplyr::slice(-index) #testing dataset

#return datasets
dataSets <- list(training = dfTrain, testing = dfTest)
invisible(dataSets)
}

#Example of using createDS()
azmDS <- createDS("./unitigs_azm.csv")
tibble::glimpse(azmDS$training[1:6])
# Rows: 2,435
# Columns: 6
# $ azm_sr <fct> 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ gene1  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ gene2  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ gene3  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ gene4  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ gene5  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
```

# Function to Train and Fit Random Forest Model (USE CAUTION)!!!
```{r}
trainMethods <- function(training, outcome, method,
                         tunegrid = NULL, tunelength=3,seed=05101929, workers = 0, number=10){
  stopifnot(is.character(method), is.object(training), is.character(outcome))
trCtrl <- caret::trainControl("cv", number = number, #cross-validation
                       allowParallel = TRUE) #parallelization

cl <- parallel::makePSOCKcluster(ifelse(workers, workers, parallel::detectCores())) #set number of cores
doParallel::registerDoParallel(cl)
set.seed(seed)

#training and fitting model
  modelRF <- caret::train(as.formula(
    paste0(outcome,"~.")),
    data=training, method=method,
    tuneGrid = tunegrid,
    tuneLength = tunelength,
    trControl = trCtrl,
    importance = T
  )
parallel::stopCluster(cl)

invisible(modelRF)
}

#Example of using trainMethods()
azmRF <- trainMethods(azmDS$training, "azm_sr", method = "parRF", tunelength = 15)
azmRF
# Parallel Random Forest 
# 
# 2435 samples
#  490 predictor
#    2 classes: '1', '0' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 2191, 2192, 2191, 2192, 2192, 2192, ... 
# Resampling results across tuning parameters:
# 
#   mtry  Accuracy   Kappa     
#     2   0.8726911  0.01630407
#    36   0.9683870  0.85560386
#    71   0.9733101  0.88246752
#   106   0.9737216  0.88456372
#   141   0.9749511  0.89045073
#   176   0.9745413  0.88863089
#   211   0.9733084  0.88305851
#   245   0.9737182  0.88498361
#   280   0.9720755  0.87744204
#   315   0.9720755  0.87744204
#   350   0.9728968  0.88054653
#   385   0.9712508  0.87334843
#   420   0.9716623  0.87535556
#   455   0.9704294  0.86932190
#   490   0.9696064  0.86605487
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was mtry = 141.
```
# Save and assess model accuracy
```{r}
saveRDS(azmRF, file = "azmRF_model") #save model
predictedAR <- predict(azmRF, azmDS$testing) #full predictions
mean(predictedAR == azmDS$testing$azm_sr) #assess full model accuracy
table(predictedAR, azmDS$testing$azm_sr) #confusion matrix
ggplot2::ggplot(azmRF)
```
#
```{r}

```

# Import and Split Data
```{r}
# #Import
# ## May require using "data_clean_script.R" to create "unitigs_cip.csv" in local directory
# cipData <- read_csv("./unitigs_cip.csv", col_types = cols(.default = col_factor(NULL))) %>% 
#   select_if(sapply(., nlevels) > 1)
# #Split
# set.seed(05101929)
# index <- cipData$cip_sr %>% 
#   createDataPartition(times = 1, p = 0.7, list = FALSE)
# 
# cipTrain <- cipData %>% slice(index)
# cipTest <- cipData %>% slice(-index)
```

# Full Model Training and Fit (DO NOT RUN!!!!)
```{r}
# trCtrl <- caret::trainControl("cv", number = 10, #cross-validation
#                        allowParallel = TRUE) #parallelization
# 
# # Select Cores for Parallelization
# cl <- parallel::makePSOCKcluster(4) #set number of cores
# doParallel::registerDoParallel(cl)
# set.seed(05101929)
# 
# #Model Fit
# modelRF <- caret::train(
#   azm_sr~., data = azm$training, method = "parRF",
#   tuneGrid = tunegrid,
#   trControl = trCtrl,
#   importance = TRUE
#   )
# #After fitting model
# 
# saveRDS(modelRF, file = "azmRF_model2") #save full model
# predictedAR <- modelRF %>% predict(azm$testing) #full predictions
# parallel::stopCluster(cl)
# mean(predictedAR == azm$testing$azm_sr) #assess full model accuracy
# 
# # Best tuning parameter
# modelRF$bestTune
```
