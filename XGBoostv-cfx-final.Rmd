---
title: "XGBoost"
author: "Valerio Tonelli Enrico"
date: "9/18/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(xgboost)
library(DiagrammeR)
library(dials)
library(workflows)
library(cvAUC)

```

```{r}
createDS <- function(csvname, percent=0.7, seed=05101929){
  stopifnot(is.character(csvname))
  
  '%>%' <- tidyr::'%>%'
  
#Import dataset
rfData <- readr::read_csv(csvname, col_types = readr::cols(.default = readr::col_factor(NULL))) %>% 
  dplyr::select_if(sapply(., nlevels) > 1)
#Split
set.seed(seed)
index <- dplyr::select(rfData,dplyr::matches(stringr::str_extract(unlist(stringr::str_split(csvname, "_"))[2],"^[[:alpha:]]+"))) %>% 
  unlist() %>% 
  caret::createDataPartition(times = 1, p = percent, list = FALSE)

dfTrain <- rfData %>% dplyr::slice(index) #training dataset
dfTest <- rfData %>% dplyr::slice(-index) #testing dataset

#return datasets
dataSets <- list(training = dfTrain, testing = dfTest)
invisible(dataSets)
}

#Example of using createDS()
cfxDS <- createDS("./unitigs_cfx.csv")
tibble::glimpse(cfxDS$training[1:6])
```
```{r}
#set.seed(111)
#dt = sort(sample(nrow(cfx), nrow(cfx)*.5))
#cfx.train<-cfx[dt,]
#cfx.test<-cfx[-dt,]
xtrain.cfx <- model.matrix (as.formula(cfx_sr~.),cfxDS$training)
xtest.cfx <- model.matrix (cfx_sr~.,cfxDS$testing)
ytrain.cfx <- cfxDS$training$cfx_sr
ytest.cfx <- cfxDS$testing$cfx_sr

#ytest.cfx <- as.numeric(ytest.cfx)
#ytrain.cfx <- as.numeric(ytrain.cfx)
ytrain.cfx <- 2 - as.numeric(ytrain.cfx)
ytest.cfx <- 2 - as.numeric(ytest.cfx)

# change to numeric values of 0 and 1s
# class(ytest.cfx)
# class(2 - as.numeric(ytest.cfx)) # numeric

#label <- as.numeric(as.character(cfx.train$cfx_sr))
```

```{r}
xgboost:::depr_par_lut
modelXGcfx <- xgboost(
  data = xtrain.cfx,
  nrounds = 777,
  label = ytrain.cfx,
  objective = "binary:logistic",
  verbose = 0
)
```
xgboost:::predict.xgb.Booster
```{r}

predictXGcfx <- predict(
  modelXGcfx, 
  xtest.cfx, 
)
# cbind(round(predictXGcfx, 2), 2 - as.numeric(ytest.cfx))
# AUC(predictXGcfx, 2 - as.numeric(ytest.cfx))

err.cfx <- mean(as.numeric(predictXGcfx > 0.5) != ytest.cfx)
print(paste("test-error=", err.cfx))
```

```{r}

predictXGcfx <- predict(
  modelXGcfx, 
  xtest.cfx, 
)
err.cfx <- mean(as.numeric(predictXGcfx > 0.5) != ytest.cfx)
print(paste("test-error=", err.cfx))
```


```{r}
tuned.cfx <- xgboost(data = xtrain.cfx, # the data           
                 max.depth = 5, # the maximum depth of each decision tree
                 nround = 100, # max number of boosting iterations
                 label = ytrain.cfx
                 )  

# generate predictions for our held-out testing data
pred.tuned.cfx <- predict(tuned.cfx, xtest.cfx)

# get & print the classification error
err.tuned.cfx <- mean(as.numeric(pred.tuned.cfx > 0.5) != ytest.cfx)
print(paste("test-error=", err.tuned.cfx))
```

```{r}
tuned.cfx <- xgboost(data = xtrain.cfx, # the data           
                 max.depth = 5, # the maximum depth of each decision tree
                 min_child_weight = 10 , #A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.
                gamma = 0.1 , #A smaller value like 0.1-0.2 can also be chosen for starting. This will anyways be tuned later.
                colsample_bytree = 0.9,  #This is a commonly used used start value. Typical values range between 0.5-0.9.
                scale_pos_weight = 1, #Because of high class imbalance.
                 nround = 777, # max number of boosting iterations
                objective = "binary:logistic",
                verbose = 0,
                 label = ytrain.cfx
                 )  

# generate predictions for our held-out testing data
pred.tuned.cfx <- predict(tuned.cfx, xtest.cfx)

# get & print the classification error
err.tuned.cfx <- mean(as.numeric(pred.tuned.cfx > 0.5) != ytest.cfx)
print(paste("test-error=", err.tuned.cfx))
#"test-error= 0.031627372052904"
```

```{r}
tuned.cfx <- xgboost(data = xtrain.cfx, # the data           
                 max.depth = 5, # the maximum depth of each decision tree
                 min_child_weight = 1 , #A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.
                gamma = 0.2 , #A smaller value like 0.1-0.2 can also be chosen for starting. This will anyways be tuned later.
                colsample_bytree = 0.9,  #This is a commonly used used start value. Typical values range between 0.5-0.9.
                scale_pos_weight = 1, #Because of high class imbalance.
                 nround = 777, # max number of boosting iterations
                 label = ytrain.cfx
                 )  

# generate predictions for our held-out testing data
pred.tuned.cfx <- predict(tuned.cfx, xtest.cfx)

# get & print the classification error
err.tuned.cfx <- mean(as.numeric(pred.tuned.cfx > 0.5) != ytest.cfx)
print(paste("test-error=", err.tuned.cfx))
#"test-error= 0.031627372052904"
```

```{r}
# get information on how important each feature is
importance_matrix <- xgb.importance(names(xtest.cfx), model = tuned.cfx)

# and plot
xgb.plot.importance(importance_matrix)
```

```{r}
xgb.plot.multi.trees(feature_names = names(xtrain.cfx), 
                     model = tuned.cfx)
```

```{r}
xgb.plot.tree(
  model = tuned.cfx, 
  )
```



























