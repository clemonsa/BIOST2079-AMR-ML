---
title: "Elastic Net Logistic Regression"
author: "Ashika Mani"
date: "9/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries
```{r}
library(tidyverse)
library(caret)
library(glmnet)
library(knitr)
library(Metrics)
```


# Import and Split Data
```{r}
createDS <- function(csvname, percent=0.7, seed=05101929){
  stopifnot(is.character(csvname))
  
  '%>%' <- tidyr::'%>%'
  
#Import dataset
rfData <- readr::read_csv(csvname, col_types = readr::cols(.default = readr::col_factor(NULL))) %>% 
  dplyr::select_if(sapply(., nlevels) > 1)
#Split
set.seed(seed)
index <- dplyr::select(rfData,dplyr::matches(stringr::str_extract(unlist(stringr::str_split(csvname, "_"))[2],"^[[:alpha:]]+"))) %>% 
  unlist() %>% 
  caret::createDataPartition(times = 1, p = percent, list = FALSE)

dfTrain <- rfData %>% dplyr::slice(index) #training dataset
dfTest <- rfData %>% dplyr::slice(-index) #testing dataset

#return datasets
dataSets <- list(training = dfTrain, testing = dfTest)
invisible(dataSets)
}

azmDS <- createDS("./unitigs_azm.csv")
cfxDS <- createDS("./unitigs_cfx.csv")
cipDS <- createDS("./unitigs_cip.csv")
```


#Finding the best lambda using cross-validation (Elastic Net)

```{r eval=FALSE, include=FALSE}

debug_contr_error <- function (dat, subset_vec = NULL) {
  if (!is.null(subset_vec)) {
    ## step 0
    if (mode(subset_vec) == "logical") {
      if (length(subset_vec) != nrow(dat)) {
        stop("'logical' `subset_vec` provided but length does not match `nrow(dat)`")
        }
      subset_log_vec <- subset_vec
      } else if (mode(subset_vec) == "numeric") {
      ## check range
      ran <- range(subset_vec)
      if (ran[1] < 1 || ran[2] > nrow(dat)) {
        stop("'numeric' `subset_vec` provided but values are out of bound")
        } else {
        subset_log_vec <- logical(nrow(dat))
        subset_log_vec[as.integer(subset_vec)] <- TRUE
        } 
      } else {
      stop("`subset_vec` must be either 'logical' or 'numeric'")
      }
    dat <- base::subset(dat, subset = subset_log_vec)
    } else {
    ## step 1
    dat <- stats::na.omit(dat)
    }
  if (nrow(dat) == 0L) warning("no complete cases")
  ## step 2
  var_mode <- sapply(dat, mode)
  if (any(var_mode %in% c("complex", "raw"))) stop("complex or raw not allowed!")
  var_class <- sapply(dat, class)
  if (any(var_mode[var_class == "AsIs"] %in% c("logical", "character"))) {
    stop("matrix variables with 'AsIs' class must be 'numeric'")
    }
  ind1 <- which(var_mode %in% c("logical", "character"))
  dat[ind1] <- lapply(dat[ind1], as.factor)
  ## step 3
  fctr <- which(sapply(dat, is.factor))
  if (length(fctr) == 0L) warning("no factor variables to summary")
  ind2 <- if (length(ind1) > 0L) fctr[-ind1] else fctr
  dat[ind2] <- lapply(dat[ind2], base::droplevels.factor)
  ## step 4
  lev <- lapply(dat[fctr], base::levels.default)
  nl <- lengths(lev)
  ## return
  list(nlevels = nl, levels = lev)
  }
```

```{r}
trainMethods <- function(training, outcome,
                         seed=05101929, workers = 0, ...){
  stopifnot(is.object(training), is.character(outcome))
trCtrl <- caret::trainControl(method = "cv", number = 10, #cross-validation
                       allowParallel = TRUE, #parallelization
                       summaryFunction = caret::twoClassSummary,
                       classProbs = TRUE) #class probabilities

cl <- parallel::makePSOCKcluster(ifelse(workers, workers, parallel::detectCores())) #set number of cores
doParallel::registerDoParallel(cl)
set.seed(seed)

#training and fitting model
  modelRF <- caret::train(as.formula(
    paste0(outcome,"~.")),
    data=training,
    trControl = trCtrl,
    metric = "ROC",
    importance = T,
    ...
  )
parallel::stopCluster(cl)

invisible(modelRF)
}

# Run trainMethods
grid <- expand.grid(.alpha = seq(0, 1, by = 0.2), .lambda = seq(0.00, 0.2, by = 0.02))
azmEN <- trainMethods(azmDS$training, "azm_sr", method = "glmnet", tuneGrid = grid)
azmEN$bestTune
```
`r azmEN$bestTune`	


# Fitting the final model on training data using best alpha and lambda
```{r}
cvfit = cv.glmnet(data.matrix(azmDS$training[,-1]), azmDS$training$azm_sr, alpha=0, family="binomial", type.measure = "mse")
lambda_1se <- cvfit$lambda.1se
# coef(cvfit,s=lambda_1se)
azm_prob <- predict(cvfit, newx =data.matrix(azmDS$testing[,-1]),s=lambda_1se,type="response")
azm_predict <- as.factor(ifelse(azm_prob > 0.5, "neg", "pos"))
confusionMatrix(azm_predict,azmDS$testing$azm_sr)
azm.mean<-mean(azm_predict==azmDS$testing$azm_sr)

# Save Model
saveRDS(azmEN, file = "azmEN_model")
```
Model Accuracy is `r azm.mean*100`%


#Finding the best alpha using cross-validation (Elastic Net)
```{r}
# Run trainMethods
cfxEN <- trainMethods(cfxDS$training, "cfx_sr", method = "glmnet", tuneGrid = grid)
cfxEN$bestTune
```
All alphas and lambda combinations give about the same accuracy level (so using a value between 0 and 1 for elastic net)

# Fitting the final model on training data using best alpha and lambda
```{r}

cvfit2 = cv.glmnet(data.matrix(cfxDS$training[,-1]), cfxDS$training$cfx_sr, alpha=0, family="binomial", type.measure = "mse")
lambda_1se <- cvfit2$lambda.1se
coef(cvfit2,s=lambda_1se)
cfx_prob <- predict(cvfit2, newx =data.matrix(cfxDS$testing[,-1]),s=lambda_1se,type="response")
cfx_predict <- as.factor(ifelse(cfx_prob > 0.5, "pos", "neg"))
confusionMatrix(cfx_predict,cfxDS$testing$cfx_sr)
cfx.mean <- mean(cfx_predict==cfxDS$testing$cfx_sr)

# Save Model
saveRDS(cfxEN, file = "cfxEN_model")
```

Model Accuracy `r cfx.mean*100`%

#Finding the best alpha using cross-validation (Elastic Net)
```{r}
# Run trainMethods
cipEN <- trainMethods(cipDS$training, "cip_sr", method = "glmnet", tuneGrid = grid)
cipEN$bestTune
```
`r cipEN$bestTune`
# Fitting the final model on training data using best alpha and lambda
```{r}
cvfit3 = cv.glmnet(data.matrix(cipDS$training[,-1]), cipDS$training$cip_sr, alpha=0.2, family="binomial", type.measure = "mse")
lambda_1se <- cvfit3$lambda.1se
coef(cvfit3,s=lambda_1se)
cip_prob <- predict(cvfit3, newx =data.matrix(cipDS$testing[,-1]),s=lambda_1se,type="response")
cip_predict <- as.factor(ifelse(cip_prob < 0.5, "neg", "pos"))
confusionMatrix(cip_predict,cipDS$testing$cip_sr)
cip.mean <- mean(cip_predict==cipDS$testing$cip_sr)

# Save Model
saveRDS(cipEN, file = "cipEN_model")
```

Model accuracy is `r cip.mean*100`%